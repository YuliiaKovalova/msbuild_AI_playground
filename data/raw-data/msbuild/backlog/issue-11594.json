{
  "number": 11594,
  "title": "[Performance]: Extremely high memory usage on virtual machine with excess resources but operates \"normally\" on a physical laptop",
  "body": "### Issue Description\n\nHi all, also posted this in one of the dotnet discords so figured I'd add that description.\n\n```\nPhysical Laptop - 16 CPUs @ 2.3 GHz, 32 GB RAM, NVMe storage (4 years old)\nKubernetes Node - 96 CPUs @ 4.1 GHz, 128 GB RAM, NVMe storage (all parts released within last 1.5 years)\n\n64 bit msbuild /t:build /m:15\n\nPhysical Laptop builds solution in 16 minutes. Idle RAM usage is about 10 GB and during build it constantly hits 100% RAM usage. Page file seems to only have 1.4 GB used by the end of the build.\n\nKubernetes Node idles around 25 GB of RAM, so everything else has 103 GB of RAM to share. Request 20 CPU and 40 GB RAM, no limit. The reason we don't limit it is due to swap memory only being utilized in a containerized environment if the container has no limit set.\n\nOne k8s pod running builds solution in 16 minutes and no page file is used. That's fine but slow-ish considering the higher clockspeed and newer hardware.\nTwo k8s pods running simultaneously on the same node results in both pods finishing the build in 24 minutes. About 5-7.5 GB page file used on the node by the end.\n\nWe're seeing drastically different RAM usage when going from the physical laptop to the k8s pods. On the physical laptop we rarely see VBCSCompiler go beyond 2 GB of RAM, page file utilization is minimal, and a single MSBuild node pretty much never goes above 1.5 GB of RAM. However in the k8s pods if only one is running it will consume a peak of 65 GB of RAM. VBCSCompiler peaks around 18-20 GB of RAM and by the end each MSBuild node is consuming 1.5-3 GB of RAM. With 2 pods running then the resources are shared and there's more than enough CPUs to share but RAM usage results in swap memory being used. That's okay, but we're surprised the build is negatively impacted so much.\n```\n\nWorth noting the Kubernetes Node scenario is also replicated on a Virtual Machine with similar resources without any of the containerization overhead. I have also tested .NET 8 vs .NET 9. While .NET 9 is better it still has very high memory usage.\n\nThe dump makes it pretty clear what is using the RAM but I'm hoping to gain some knowledge as to why the RAM is allocated to see if we can somehow improve the situation on our end. The Gen2 survival rate is >90% for these MSBuild processes. \n\n### Steps to Reproduce\n\nI cannot provide reproduce steps due to this occurring against internal solutions and projects. I don't think I'll be able to reproduce the problem easily either. \n\n### Data\n\nI have ETW and Dumps but cannot upload that to a public forum. If needed I can open a Visual Studio ticket and provide the information there.\n\n### Analysis\n\nI've been given advice from multiple folks on the dotnet Discord and have generated data for most of what was recommended.\n\nTry .NET 9 SDK instead of .NET 8: There was an improvement in total RAM used but it was in the low single digits. Not really enough to make a dent in the performance we're seeing.\n\nGenerate Microsoft.Build ETW: I gathered this data but struggle to understand how to use it to determine _what_ is causing the Gen2 GC to have a >90% survival rate. I'm a beginner at best with the tool and certain things cannot be opened like GC Heap Stacks. I assume it's due to too the ETW being too large but haven't dug too deep. \n\nGenerate dump files: This has helped in identifying exactly where the RAM is going, but my lack of deep MSBuild knowledge makes it difficult to track down the why.\n\nThis is an example of one of the MSBuild nodes consuming >1 GB. Towards the end of a build a command with /m:15 will have 15 nodes at 2.5-3 GB of RAM allocated to each.\n\n![Image](https://github.com/user-attachments/assets/00624dc1-d0a0-4ccf-bab4-5d6b7d537c82)\n\n\n\n### Versions & Configurations\n\n.NET 8 - 8.0.404\n.NET 9 - 9.0.101\n\n### Regression\n\n- [ ] yes\n- [ ] no\n\n### Regression Details\n\n_No response_",
  "state": "CLOSED",
  "createdAt": "2025-03-14T11:31:16Z",
  "updatedAt": "2025-05-16T13:47:55Z",
  "closedAt": "2025-05-16T13:47:53Z",
  "author": {
    "login": "adc-cjewett"
  },
  "milestone": null,
  "assignees": {
    "nodes": [
      {
        "login": "SimaTian"
      }
    ]
  },
  "labels": [
    "Area: Performance",
    "needs-more-info",
    "Priority:2",
    "stale",
    "closed-by-bot",
    "triaged"
  ],
  "comments": {
    "nodes": [
      {
        "body": "Hello, can you help us by uploading the logs and traces via the ticket please? \nAlso, can you generate a [binlog](aka.ms/binlog) from a run and upload it via a community post?\nPlease use a community ticket to ensure data privacy. (More info in the link above)\n\nIf we don't have the reproduction, these will give us the best shot at attempting to figure out what is wrong.\n\nSome general performance tips:\n -  [setting up a dev drive can help]( https://learn.microsoft.com/en-us/windows/dev-drive/)\n -  [adding exceptions to windows defender can help](https://developercommunity.visualstudio.com/t/Significant-loading-time-while-starting-/10824119#T-N10845571)\nPlease note that these are only generic performance tips that don't target RAM specifically.\n",
        "createdAt": "2025-03-17T12:30:29Z",
        "author": {
          "login": "SimaTian"
        }
      },
      {
        "body": "Thanks for the response @SimaTian! I'll let you know once that ticket is created.\nI'll upload the binlog and other data upon request in that ticket. ",
        "createdAt": "2025-03-18T00:50:19Z",
        "author": {
          "login": "adc-cjewett"
        }
      },
      {
        "body": "This issue is marked as stale because feedback has been requested for 30 days with no response. Please respond within 14 days or this issue will be closed due to inactivity.\n<!-- Policy app identification https://img.shields.io/static/v1?label=PullRequestIssueManagement. -->",
        "createdAt": "2025-05-01T20:38:18Z",
        "author": {
          "login": "dotnet-policy-service"
        }
      },
      {
        "body": "This issue was closed due to inactivity. If you can still reproduce this bug, please comment with the requested information, detailed steps to reproduce the problem, or any other notes that might help in the investigation.\n<!-- Policy app identification https://img.shields.io/static/v1?label=PullRequestIssueManagement. -->",
        "createdAt": "2025-05-16T13:47:54Z",
        "author": {
          "login": "dotnet-policy-service"
        }
      }
    ]
  }
}