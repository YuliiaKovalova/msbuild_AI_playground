{
  "number": 10036,
  "title": "[Infra] SampleAnalyzerIntegrationTest randomly timeouting in CI",
  "body": "### Context\r\n\r\n```\r\nSystem.TimeoutException : Test failed due to timeout: process 12042 is active for more than 30 sec.\r\n\r\n   at Microsoft.Build.UnitTests.Shared.RunnerUtilities.RunProcessAndGetOutput(String process, String parameters, Boolean& successfulExit, Boolean shellExecute, ITestOutputHelper outputHelper) in /_/src/UnitTests.Shared/RunnerUtilities.cs:line 139\r\n   at Microsoft.Build.UnitTests.Shared.RunnerUtilities.ExecBootstrapedMSBuild(String msbuildParameters, Boolean& successfulExit, Boolean shellExecute, ITestOutputHelper outputHelper) in /_/src/UnitTests.Shared/RunnerUtilities.cs:line 67\r\n   at Microsoft.Build.BuildCheck.UnitTests.EndToEndTests.SampleAnalyzerIntegrationTest(Boolean buildInOutOfProcessNode, Boolean analysisRequested) in /home/vsts/work/1/s/src/BuildCheck.UnitTests/EndToEndTests.cs:line 122\r\n   at System.RuntimeMethodHandle.InvokeMethod(Object target, Void** arguments, Signature sig, Boolean isConstructor)\r\n   at System.Reflection.MethodBaseInvoker.InvokeDirectByRefWithFewArgs(Object obj, Span`1 copyOfArgs, BindingFlags invokeAttr)\r\n```\r\n\r\nSample run: https://dev.azure.com/dnceng-public/public/_build/results?buildId=648080&view=ms.vss-test-web.build-test-results-tab&runId=16013174&resultId=100001&paneView=debug",
  "state": "CLOSED",
  "createdAt": "2024-04-18T15:34:37Z",
  "updatedAt": "2024-07-23T13:27:35Z",
  "closedAt": "2024-07-23T13:27:35Z",
  "author": {
    "login": "JanKrivanek"
  },
  "labels": [
    "Area: Our Own Build",
    "needs-investigation",
    "Area: BuildCheck"
  ],
  "assignees": {
    "nodes": [
      {
        "login": "JanKrivanek"
      },
      {
        "login": "YuliiaKovalova"
      }
    ]
  },
  "milestone": null,
  "comments": {
    "nodes": [
      {
        "body": "discussed offline: the execution time can be extended for handling it. Please track it for some time if this approach solves the problem",
        "createdAt": "2024-04-19T09:10:38Z",
        "updatedAt": "2024-04-19T09:14:37Z",
        "author": {
          "login": "YuliiaKovalova"
        }
      },
      {
        "body": "After observing 9 runs that failed with this test, the failure only happened to the following test cases. From the output logs attached inline, it looks like within 30 seconds the build check analyzer hadn't finished.\r\n- SampleAnalyzerIntegrationTest(buildInOutOfProcessNode: True, analysisRequested: True) - Fail  [buildInOutOfProcessNode_True-analysisRequested_True.txt](https://github.com/dotnet/msbuild/files/15160870/buildInOutOfProcessNode_True-analysisRequested_True.txt)\r\n- SampleAnalyzerIntegrationTest(buildInOutOfProcessNode: False, analysisRequested: True) - Fail [buildInOutOfProcessNode_False-analysisRequested_True.txt](https://github.com/dotnet/msbuild/files/15160885/buildInOutOfProcessNode_False-analysisRequested_True.txt)\r\n\r\n~~Checking with the output log [Microsoft.Build.BuildCheck.UnitTests_net8.0_x64.log](https://github.com/dotnet/msbuild/files/15160911/Microsoft.Build.BuildCheck.UnitTests_net8.0_x64.log) of fully passed test run, I found there were over 1000 warnings when the analysis was requested. But the warnings except `BC0101` couldn't be reproduced locally. This may explain it looks like within 30 seconds the build check analyzer hadn't finished. No idea why CI run has so many extra warnings.~~\r\n\r\n\r\n",
        "createdAt": "2024-04-30T09:10:54Z",
        "updatedAt": "2024-05-06T09:22:01Z",
        "author": {
          "login": "GangWang01"
        }
      },
      {
        "body": "@JanKrivanek , can this fix help with the test performance?\r\nhttps://github.com/dotnet/msbuild/pull/10084",
        "createdAt": "2024-04-30T09:17:15Z",
        "updatedAt": "2024-04-30T09:17:15Z",
        "author": {
          "login": "YuliiaKovalova"
        }
      },
      {
        "body": "Only slightly - it slashes only about a third of currently detected slowdown of the analyzers.\r\nThough the detected slowdown was in range of about 14% - so it's puzzling that the process runs for over 30 second. It would be interesting to see if it fails with about the same or significantly decreased ratio if we increase the timeout to say - 120 seconds. That might bisect between the slowdown and deadlock theory",
        "createdAt": "2024-04-30T09:47:02Z",
        "updatedAt": "2024-04-30T09:47:02Z",
        "author": {
          "login": "JanKrivanek"
        }
      },
      {
        "body": "Another incidence (after the timeout increasing): https://dev.azure.com/dnceng-public/public/_build/results?buildId=681320&view=ms.vss-test-web.build-test-results-tab&runId=16909114&resultId=100003&paneView=debug",
        "createdAt": "2024-05-20T15:33:43Z",
        "updatedAt": "2024-05-20T15:33:43Z",
        "author": {
          "login": "JanKrivanek"
        }
      },
      {
        "body": "Might be totally red herring:\r\n\r\nThe timeouting cases seem to have TerminalLogger used (notice the control sequences):\r\n\r\n```\r\n\u001b[33;1m   Microsoft.Build.BuildCheck.UnitTests: [Long Running Test] 'Microsoft.Build.BuildCheck.UnitTests.EndToEndTests.SampleAnalyzerIntegrationTest', Elapsed: 00:01:02\r\n\u001b[mExecuting [/home/vsts/work/1/s/.dotnet/dotnet /home/vsts/work/1/s/artifacts/bin/bootstrap/net8.0/MSBuild/MSBuild.dll FooBar.csproj /m:1 -nr:False -restore -analyze]\r\n==== OUTPUT ====\r\n\u001b[31;1m    Microsoft.Build.BuildCheck.UnitTests.EndToEndTests.SampleAnalyzerIntegrationTest(buildInOutOfProcessNode: True, analysisRequested: True) [FAIL]\r\n\u001b[m\u001b[37m      System.TimeoutException : Test failed due to timeout: process 11430 is active for more than 120 sec.\r\n```\r\n\r\nSample full log: [TIMEOUT-Microsoft.Build.BuildCheck.UnitTests_net8.0_x64.log](https://github.com/dotnet/msbuild/files/15380508/TIMEOUT-Microsoft.Build.BuildCheck.UnitTests_net8.0_x64.log)\r\n\r\nWherease I do not see those appear in individual test cases for the success cases - sample log: [OK-Microsoft.Build.BuildCheck.UnitTests_net8.0_x64.log](https://github.com/dotnet/msbuild/files/15380480/OK-Microsoft.Build.BuildCheck.UnitTests_net8.0_x64.log)\r\n\r\n\r\nRegardless of it being red herring or not - it is strange that TerminalLogger is used in CI. I believe @AR-May was looking into somthing similar\r\n",
        "createdAt": "2024-05-20T18:15:45Z",
        "updatedAt": "2024-05-20T18:15:45Z",
        "author": {
          "login": "JanKrivanek"
        }
      },
      {
        "body": "@JanKrivanek  and @maridematte  I repro this issue with the command \"dotnet test ./artifacts/bin/Microsoft.Build.BuildCheck.UnitTests/Debug/net8.0/Microsoft.Build.BuildCheck.UnitTests.dll --filter \"FullyQualifiedName~SampleAnalyzerIntegrationTest\" randomly\n\"  in linux machine.  \n\n Added -v:d with process command to output more info and found the build is stuck in target GetCopyToPublishDirectoryItems\n\n\n![Image](https://github.com/dotnet/msbuild/assets/26814373/53c42ba4-a63d-4908-af83-b57599aeb5eb)\n\n\nThis is build [log.txt](https://github.com/dotnet/msbuild/files/15433953/log.txt). \n\n",
        "createdAt": "2024-05-24T12:11:41Z",
        "updatedAt": "2024-05-24T12:14:40Z",
        "author": {
          "login": "JaynieBai"
        }
      },
      {
        "body": "Perfect!\r\n\r\nCan you collect 2 more cases to see if there is any pattern? Plus appart from the diag log it might be helpfull to have binlog as well.\r\n\r\nOther than that I unfortunately do not have any idea now about what can bw causing this",
        "createdAt": "2024-05-24T13:41:44Z",
        "updatedAt": "2024-05-24T13:41:44Z",
        "author": {
          "login": "JanKrivanek"
        }
      },
      {
        "body": "> Perfect!\r\n> \r\n> Can you collect 2 more cases to see if there is any pattern? Plus appart from the diag log it might be helpfull to have binlog as well.\r\n> \r\n> Other than that I unfortunately do not have any idea now about what can bw causing this\r\n\r\nHere is log with -v:diag  [diaglog.txt](https://github.com/dotnet/msbuild/files/15479392/diaglog.txt)\r\n\r\n\r\nThe binlog is not completed when failed.  [True.binlog.txt](https://github.com/dotnet/msbuild/files/15478527/True.binlog.txt)\r\n\r\n![Image](https://github.com/dotnet/msbuild/assets/26814373/acb14217-4e0d-4bb7-b470-71e371aeabe0)\r\n\r\n\r\n\r\n\r\n\r\n",
        "createdAt": "2024-05-29T05:55:45Z",
        "updatedAt": "2024-05-31T04:10:17Z",
        "author": {
          "login": "JaynieBai"
        }
      },
      {
        "body": "Yeah - the binlog being cut aburptly is fine (and expected as test is killed after timeout) - the timeline from binlog will be interresting - it should show where was the time spent. At it'd be nice to see 2 or 3 cases - to be able to compare if the excesive time is spent in the same part of the build or if it is random.\r\n\r\nSince it is Linux, we unfortunately cannot use ETW :-/\r\nMaybe @AR-May  or @ladipro have some top of the head ideas of what to focus on beyond the timing in binlog.",
        "createdAt": "2024-05-30T08:17:52Z",
        "updatedAt": "2024-05-30T08:17:52Z",
        "author": {
          "login": "JanKrivanek"
        }
      },
      {
        "body": "> Yeah - the binlog being cut aburptly is fine (and expected as test is killed after timeout) - the timeline from binlog will be interresting - it should show where was the time spent. At it'd be nice to see 2 or 3 cases - to be able to compare if the excesive time is spent in the same part of the build or if it is random.\r\n> \r\n> Since it is Linux, we unfortunately cannot use ETW :-/ Maybe @AR-May or @ladipro have some top of the head ideas of what to focus on beyond the timing in binlog.\r\n\r\nI compare the logs of several failed cases, it's always the same part of the build.  And unfortunately, there is no timeline shown in the binlog [True.binlog.txt](https://github.com/dotnet/msbuild/files/15478527/True.binlog.txt)",
        "createdAt": "2024-05-31T04:12:03Z",
        "updatedAt": "2024-05-31T04:13:40Z",
        "author": {
          "login": "JaynieBai"
        }
      },
      {
        "body": "From offline discussion\r\n\r\nObservations:\r\n * The repros seem to indicate that the test will get stuck in the same place - during [`_GenerateRestoreProjectPathWalk`](https://github.com/dotnet/dotnet/blob/91ab1b9027707137f2fd2f0541c9f2d23e958a29/src/nuget-client/src/NuGet.Core/NuGet.Build.Tasks/NuGet.targets#L1250) execution (which is invoked from restore) - on a FooBar-Copy.csproj (the second project) - while the previous call for a same project succeeded (from a recursive call of `_GenerateRestoreProjectPathWalk` on the FooBar.csproj)\r\n * The target is calling MSBuild task (to execute the same target recursively) - but that MSBuild call doesn't end\r\n\r\nRecommendation on further steps:\r\n * Let's setup WSL unit testing (https://github.com/dotnet/msbuild/blob/9bea8026aad964cb36f3ec9d93bd95a941487690/documentation/wiki/Building-Testing-and-Debugging-on-Full-Framework-MSBuild.md?plain=1#L36)\r\n * Let's see if the hang reproes under the WSL testing (if it doesn't we can try with docker described in the same doc)\r\n * Let's setup WSL debugging https://learn.microsoft.com/en-us/visualstudio/debugger/debug-dotnet-core-in-wsl-2?view=vs-2022\r\n * Run the test and once it hangs, break into the child MSBuild process from Debug -> Attach to Process\r\n * Inspect the stacks to see if there is any blocking\r\n * If the stack inspection and debugging won't help - we might need to try to collect traces: https://github.com/ltrzesniewski/dotnet-runtime/blob/master/docs/project/linux-performance-tracing.md\r\n\r\n\r\n\r\nAs a side not - the issue happens during restore. So it might get resolved by skipping the restore - https://github.com/dotnet/msbuild/issues/9747. But it still would be very valuable to understand why is it happening",
        "createdAt": "2024-06-05T11:38:33Z",
        "updatedAt": "2024-06-05T11:38:33Z",
        "author": {
          "login": "JanKrivanek"
        }
      },
      {
        "body": "Though this issue might be resolved in the fix above, I had some information about trying to debug the test running on linux remotely from VS on Windows. I tried the following ways. But they failed with different errors. I was not able to debug this test running on Linux.\r\n- Debug the test in WSL\r\nRemote debugging in WSL got the error _MSB4062: The \"CheckForImplicitPackageReferenceOverrides\" task could not be loaded from the assembly /mnt/d/WS/fork/msbuild/artifacts/bin/bootstrap/net8.0/MSBuild/Sdks/Microsoft.NET.Sdk/targets/../tools/net8.0/Microsoft.NET.Build.Tasks.dll. An attempt was made to load a program with an incorrect format._ See \r\n[TestLog.txt](https://github.com/user-attachments/files/16231849/TestLog.txt) for more details. After my investigation, I found the assembly Microsoft.NET.Build.Tasks.dll existed. But this assembly from clean build on Windows is not compatible to run in WSL due to its architecture x64, while the architecture of it from clean build on Linux is 64797.\r\n\r\n- Attach the debugger to the child process started by the test running on Linux\r\nI could attach the debugger to the started process(dotnet <MSBuild.dll in boostrap>) by the test on Linux for a while. There is was no call  stack displayed and after a while the error below popped up. No Windows application event log was found either...\r\n![image](https://github.com/user-attachments/assets/8435737c-2b99-48a3-8e31-421081f85f61)\r\n\r\n\r\n",
        "createdAt": "2024-07-15T09:22:14Z",
        "updatedAt": "2024-07-15T09:22:14Z",
        "author": {
          "login": "GangWang01"
        }
      },
      {
        "body": "> Though this issue might be resolved in the fix above, I had some information about trying to debug the test running on linux remotely from VS on Windows. I tried the following ways. But they failed with different errors. I was not able to debug this test running on Linux.\r\n> \r\n> * Debug the test in WSL\r\n>   Remote debugging in WSL got the error _MSB4062: The \"CheckForImplicitPackageReferenceOverrides\" task could not be loaded from the assembly /mnt/d/WS/fork/msbuild/artifacts/bin/bootstrap/net8.0/MSBuild/Sdks/Microsoft.NET.Sdk/targets/../tools/net8.0/Microsoft.NET.Build.Tasks.dll. An attempt was made to load a program with an incorrect format._ See\r\n>   [TestLog.txt](https://github.com/user-attachments/files/16231849/TestLog.txt) for more details. After my investigation, I found the assembly Microsoft.NET.Build.Tasks.dll existed. But this assembly from clean build on Windows is not compatible to run in WSL due to its architecture x64, while the architecture of it from clean build on Linux is 64797.\r\n> * Attach the debugger to the child process started by the test running on Linux\r\n>   I could attach the debugger to the started process(dotnet <MSBuild.dll in boostrap>) by the test on Linux for a while. There is was no call  stack displayed and after a while the error below popped up. No Windows application event log was found either...\r\n>   ![image](https://private-user-images.githubusercontent.com/2950449/348654603-8435737c-2b99-48a3-8e31-421081f85f61.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjEwMzU4NjUsIm5iZiI6MTcyMTAzNTU2NSwicGF0aCI6Ii8yOTUwNDQ5LzM0ODY1NDYwMy04NDM1NzM3Yy0yYjk5LTQ4YTMtOGUzMS00MjEwODFmODVmNjEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDcxNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA3MTVUMDkyNjA1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWJkOWY3MmNkYTVkYTE0YTQ1OTgxODVjMzAyYTZjNjU3ZmZjZDZhYjMzMmMwYmQxOThlNTlmYjQ2ZjI3OGQ2NCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.ny443gYy42IksRVIDtgaQ1tCynYDuGziUFoVwH2dqR0)\r\n\r\n@GangWang01 , thank you for analysis.\r\nI was able to catch it on Windows locally, after adding a new build-in rule. Using Parallel.ForEach in this case was causing deadlock, according to the callstack. So it has to be OK now.\r\nCould you please reenable to tests in a separate PR now? I included it in my changes, but the review can take time :)",
        "createdAt": "2024-07-15T09:28:15Z",
        "updatedAt": "2024-07-15T09:28:15Z",
        "author": {
          "login": "YuliiaKovalova"
        }
      },
      {
        "body": "Fixed by https://github.com/dotnet/msbuild/pull/10353",
        "createdAt": "2024-07-23T13:27:35Z",
        "updatedAt": "2024-07-23T13:27:35Z",
        "author": {
          "login": "JanKrivanek"
        }
      }
    ]
  }
}