{
  "number": 7086,
  "title": "Hash.Execute() allocates a string which gets to the large object heap.",
  "body": "I noticed that `Hash.Execute()` sometimes allocates a big string.\r\n![image](https://user-images.githubusercontent.com/67507805/143437377-d870c8a8-21ac-47fd-961d-a94590f14251.png)\r\n\r\nIt happens because we the first join all the items into one string and then apply hashing algorithm. \r\nIt would be nice to try hash it one by one or use a buffer with fixed length to break this big string into chunks.\r\n\r\nAdditional info:\r\nThere was an attempt to improve this function already: #5560",
  "state": "CLOSED",
  "createdAt": "2021-11-25T12:17:11Z",
  "updatedAt": "2024-02-21T17:00:52Z",
  "closedAt": "2022-01-21T08:30:09Z",
  "author": {
    "login": "AR-May"
  },
  "labels": [
    "Area: Performance",
    "triaged"
  ],
  "assignees": {
    "nodes": [
      {
        "login": "AR-May"
      }
    ]
  },
  "milestone": null,
  "comments": {
    "nodes": [
      {
        "body": "You might use HashCode.Combine. It's in box for .NET Core 2.1+ but it's available in a NuGet package for .NET Framework so no multi targeting is necessary.",
        "createdAt": "2021-11-30T15:15:22Z",
        "updatedAt": "2021-11-30T15:15:22Z",
        "author": {
          "login": "danmoseley"
        }
      },
      {
        "body": "Thank you for the suggestion. I looked at `HashCode` struct.\r\nI think that collision rate of hashing algorithm in `HashCode` might not suffice for our goals. We need it to be low, because, as far as I know, there is no further verification - if hashes coincide we think that objects (list of files for compilation) coincide too. At this moment sha1 is used (and we do not care about it (not) being cryptographic, rather caring about it's collision ratio) and switching to `HashCode` may lead to regressions. \r\nI also have not seen an option to set a random seed for `HashCode`. We need the algorithm to be stable between incremental builds.",
        "createdAt": "2021-11-30T18:01:56Z",
        "updatedAt": "2021-11-30T18:05:57Z",
        "author": {
          "login": "AR-May"
        }
      },
      {
        "body": "> If hashes coincide we think that objects (list of files for compilation) coincide too.\r\n\r\nIf you're relying on different objects to have different hash codes, that's not hashing and hash codes should not be used for this. You need some kind of unique identifier.\r\n\r\nIf your goal is to just have a measurably small rate of collisions, hopefully a failure just results in lower efficiency rather than incorrect behavior. If this is your goal yes you would want to use a sufficiently large cryptographic hash, not a regular hash code as it has no particular guarantees and could choose to generate poorly distributed codes for efficiency reasons.\r\n\r\n> I also have not seen an option to set a random seed for HashCode. We need the algorithm to be stable between incremental builds.\r\n\r\nAs far as I'm aware, all the hash code generation in the core libraries is stable except for String. We randomize the hash codes of string by default, to make DOS attacks more difficult. Of course it is possible to generate your own hashcodes for strings, if you need stable ones.",
        "createdAt": "2021-11-30T18:36:59Z",
        "updatedAt": "2021-11-30T18:36:59Z",
        "author": {
          "login": "danmoseley"
        }
      },
      {
        "body": "Well, using hash for such goals is current behavior and initial goal of this issue was just to remove unnecessary LOH allocations in hash computations rather than rethinking the whole approach. \r\n\r\nThinking of the usage of hash function, from one point of view, I agree that such usage of hash function is not quite common and indeed might be dangerous. Collision, as far as i know, will result in build error and the behavior of MSBuild thus would be not correct. \r\nFrom another point of view, probability of the collision is really very small with sha1. Unique identifiers at the same time sometimes are big enough to get in Large Object Heap, we do not want to save and work with them unless necessary. \r\nIf the build fails it will not be a huge problem also - we will just need to use rebuild instead of incremental build. After that we will not get further failures, so the error will not be consistent.  \r\n\r\ncc @rainersigwald \r\nWhat do you think about that? Also, am i right with my current understanding of the usage of hash task? ",
        "createdAt": "2021-12-01T19:43:10Z",
        "updatedAt": "2021-12-01T19:43:10Z",
        "author": {
          "login": "AR-May"
        }
      },
      {
        "body": "The cost of a collision here is silent incorrect underbuild, which is pretty bad as build errors go but was deemed to be acceptable for this case, especially since we shouldn't get any particularly adversarial input. As you say, the workaround is to do a full build.\r\n\r\nWe're looking at options to improve this, for example #7043. For now I think you're on a fine track @AR-May.",
        "createdAt": "2021-12-01T20:28:55Z",
        "updatedAt": "2021-12-01T20:28:55Z",
        "author": {
          "login": "rainersigwald"
        }
      }
    ]
  }
}