{
  "number": 1308,
  "title": "Extreme slowdowns when process memory usage is high",
  "body": "Whenever MSBuild completes a `BuildRequest`, it [checks](https://github.com/Microsoft/msbuild/blob/4e405a802d587368581117daf3402491beea8de7/src/XMakeBuildEngine/BackEnd/Components/BuildRequestEngine/BuildRequestEngine.cs#L771) to see if the current process is using \"too much\" memory (defined as 80% of the virtual memory limit). If it is, the engine [attempts to free memory](https://github.com/Microsoft/msbuild/blob/4e405a802d587368581117daf3402491beea8de7/src/XMakeBuildEngine/BackEnd/Components/BuildRequestEngine/BuildRequestEngine.cs#L854-L862) by flushing build results to a file-based cache and calling for garbage collection.\r\n\r\nThis system likely worked well at some point, when process memory usage came primarily from MSBuild and flushing results to disk released a significant amount of memory. But if the bulk of the memory usage is coming from something _else_ in the process, this can cause extreme slowdowns: every build request completion does synchronous I/O _and_ a stop-the-world GC. This is especially painful if the process in question is `devenv.exe` and it's in the midst of a solution-configuration change (i.e. \"do a design-time build of every project in the solution\").",
  "state": "OPEN",
  "createdAt": "2016-11-03T19:26:35Z",
  "updatedAt": "2024-02-21T16:37:59Z",
  "closedAt": null,
  "author": {
    "login": "rainersigwald"
  },
  "labels": [
    "backlog",
    "Area: Performance",
    "triaged"
  ],
  "assignees": {
    "nodes": []
  },
  "milestone": {
    "title": "Backlog"
  },
  "comments": {
    "nodes": [
      {
        "body": "Possible solutions:\n- Just remove this code.\n  - Hard to judge the impact, but building very large solutions + project trees in VS and command-line MSBuild could give pretty good confidence.\n  - It'd be nice to have telemetry on how often we hit this situation in the first place and how well it works when we do: even if it's common, if we're not freeing significant memory, there's no point in doing this.\n- Add some sort of frequency check to the try-to-free-memory code and don't attempt more than every N requests.\n- Add some sort of efficacy check to the try-to-free-memory process and stop trying if previous attempts didn't free much memory.\n",
        "createdAt": "2016-11-03T19:30:11Z",
        "updatedAt": "2016-11-03T19:30:11Z",
        "author": {
          "login": "rainersigwald"
        }
      },
      {
        "body": "Another possibility:\n- Only do this if the host process is MSBuild.exe\n  - On the theory that giant builds are command line builds, and if we're the only thing running we're much more likely to be the cause of memory overconsumption.\n  - Whereas devenv.exe or another host can have its own thing going on, potentially with many plugins, and plenty of other opportunities for memory hogging.\n  - Not clean but cheap and easy.\n",
        "createdAt": "2016-11-04T15:52:12Z",
        "updatedAt": "2016-11-04T15:52:12Z",
        "author": {
          "login": "rainersigwald"
        }
      },
      {
        "body": "This was necessary in order to build the VS tree successfully (before it was pared down and broken up). I like your idea of making it a feature the host opts into, and MSBuild.exe would opt into it and devenv would not. \n",
        "createdAt": "2016-11-09T22:49:29Z",
        "updatedAt": "2016-11-09T22:49:29Z",
        "author": {
          "login": "danmoseley"
        }
      },
      {
        "body": "@Forgind to make sure we add a code marker for this.",
        "createdAt": "2019-11-04T21:33:00Z",
        "updatedAt": "2019-11-04T21:33:00Z",
        "author": {
          "login": "livarcocc"
        }
      },
      {
        "body": "How high priority is adding an event for this? If it's not too high, I can add it to #4725 instead.",
        "createdAt": "2019-11-04T22:10:47Z",
        "updatedAt": "2019-11-04T22:10:47Z",
        "author": {
          "login": "Forgind"
        }
      },
      {
        "body": "This probably is not the best solution, but what would you think of:\r\nTry to write configurations to disk and collect garbage. If memory freed < some small fraction of total virtual memory, throw a \"need more memory\" exception.",
        "createdAt": "2019-12-11T00:25:19Z",
        "updatedAt": "2019-12-11T00:25:19Z",
        "author": {
          "login": "Forgind"
        }
      },
      {
        "body": "I don't think we should ever throw our own OOM. \"Running out of memory\" isn't the same as \"fully out of memory\" and I don't think it helps to push the failure forward in time.",
        "createdAt": "2019-12-11T20:29:33Z",
        "updatedAt": "2019-12-11T20:29:33Z",
        "author": {
          "login": "rainersigwald"
        }
      },
      {
        "body": "What about having a variable that starts at 0.8 and increases each time we try and fail to reclaim a significant amount of memory? In other words, if memory freed is small, we bump the point at which we try again to 0.9, then 0.95 if we fail again, and finally 1.0.",
        "createdAt": "2019-12-11T21:54:03Z",
        "updatedAt": "2019-12-11T21:54:03Z",
        "author": {
          "login": "Forgind"
        }
      },
      {
        "body": "That's a good idea, but I don't think it's currently worth the effort.",
        "createdAt": "2019-12-11T22:43:22Z",
        "updatedAt": "2019-12-11T22:43:22Z",
        "author": {
          "login": "rainersigwald"
        }
      },
      {
        "body": "Perf triage: Looks like the issue is still valid but the priority is low. If it sits in the backlog long enough, it may end up turning into \"cleanup: remove dead 32-bit only code\" \ud83d\ude1b ",
        "createdAt": "2021-04-12T13:39:04Z",
        "updatedAt": "2021-04-12T13:39:04Z",
        "author": {
          "login": "ladipro"
        }
      }
    ]
  }
}