{
  "number": 6023,
  "title": "Reduce byte array allocations when reading/writing packets",
  "body": "Byte arrays are a major source of LOH allocations when streaming logging events across nodes. Allocating a large MemoryStream once and then growing it as needed almost completely removes allocations for byte arrays.\r\n\r\nThis should significantly improve memory traffic during large builds.\r\n\r\n### Before:\r\n![image](https://user-images.githubusercontent.com/679326/104253706-ff62f600-5429-11eb-892a-ff8747038347.png)\r\n\r\n### After:\r\n![image](https://user-images.githubusercontent.com/679326/104253730-0f7ad580-542a-11eb-9d1b-d84df5b1c6a4.png)\r\n",
  "state": "MERGED",
  "createdAt": "2021-01-12T00:30:13Z",
  "updatedAt": "2021-02-09T00:24:30Z",
  "closedAt": "2021-02-09T00:24:25Z",
  "mergedAt": "2021-02-09T00:24:25Z",
  "additions": 119,
  "deletions": 58,
  "changedFiles": 2,
  "headRefName": "dev/kirillo/memoryStream",
  "isDraft": false,
  "author": {
    "login": "KirillOsenkov"
  },
  "milestone": null,
  "assignees": {
    "nodes": []
  },
  "labels": [
    "Area: Performance",
    "merge-when-branch-open"
  ],
  "commits": {
    "nodes": [
      {
        "commit": {
          "oid": "b7dde7de4269b70701b57e07e90ddbedddb1185e",
          "message": "Reduce byte array allocations when reading/writing packets\n\nByte arrays are a major source of LOH allocations when streaming logging events across nodes. Allocating a large MemoryStream once and then growing it as needed almost completely removes allocations for byte arrays.\n\nThis should significantly improve memory traffic during large builds.",
          "committedDate": "2021-01-11T21:26:36Z",
          "author": {
            "name": "Kirill Osenkov",
            "email": "github@osenkov.com"
          }
        }
      },
      {
        "commit": {
          "oid": "6eb445dc4128819b50094b37760388a135f28452",
          "message": "Write the last part of the packet synchronously on Mono.\n\nIf we use WriteAsync and don't await it, then subsequent WriteAsync may be called before the first continuation returns. If both calls share the same buffer and they overlap, we will overwrite the data in the buffer and cause junk to arrive at receiver.",
          "committedDate": "2021-01-17T07:31:55Z",
          "author": {
            "name": "Kirill Osenkov",
            "email": "github@osenkov.com"
          }
        }
      },
      {
        "commit": {
          "oid": "8dea4796cadcb7880beb9177291e81739045cee9",
          "message": "Remove default MemoryStream size.",
          "committedDate": "2021-01-25T01:16:20Z",
          "author": {
            "name": "Kirill Osenkov",
            "email": "github@osenkov.com"
          }
        }
      },
      {
        "commit": {
          "oid": "7f6b35865a0907c074cc8344e63aba1aa4f6704c",
          "message": "Get rid of BinaryWriter.",
          "committedDate": "2021-01-25T01:30:20Z",
          "author": {
            "name": "Kirill Osenkov",
            "email": "github@osenkov.com"
          }
        }
      },
      {
        "commit": {
          "oid": "c89545c34ebc3fcd380fbc404ec4e8dbdb4279f9",
          "message": "Make SendData write packets asynchronously.\n\nThis avoids blocking the main loop. Roughly equivalent to writing the packet asynchronously using fire-and-forget (BeginWrite).",
          "committedDate": "2021-01-25T02:27:08Z",
          "author": {
            "name": "Kirill Osenkov",
            "email": "github@osenkov.com"
          }
        }
      },
      {
        "commit": {
          "oid": "ccfec12ccb65fb8e60413f5b2ee777a9b829b268",
          "message": "Introduce local to simplify the diff.",
          "committedDate": "2021-01-25T03:25:46Z",
          "author": {
            "name": "Kirill Osenkov",
            "email": "github@osenkov.com"
          }
        }
      },
      {
        "commit": {
          "oid": "4ba9f1a1b007ed527272e1441f082420f738a821",
          "message": "Add comments.",
          "committedDate": "2021-01-26T20:00:07Z",
          "author": {
            "name": "Kirill Osenkov",
            "email": "github@osenkov.com"
          }
        }
      },
      {
        "commit": {
          "oid": "5eca0431b40c7be3e918880720ed1f4058f4bf5e",
          "message": "Added a clarification.",
          "committedDate": "2021-01-28T22:31:51Z",
          "author": {
            "name": "Kirill Osenkov",
            "email": "github@osenkov.com"
          }
        }
      }
    ]
  },
  "comments": {
    "nodes": [
      {
        "body": "This should significantly improve memory pressure during large builds: \r\nhttps://github.com/dotnet/msbuild/issues/2168\r\n\r\nStrings and char[] arrays will still be allocated so it's not a full solution. But some low hanging fruit that I hope will bring some improvement.\r\n\r\n@mmitche FYI",
        "createdAt": "2021-01-12T00:31:42Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "I'm looking at Mono failures... presumably I broke something around FEATURE_APM... a packet arrives, of the right length, but contains junk...",
        "createdAt": "2021-01-12T01:24:13Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "Part 3 works, everything else fails.\r\n\r\nThis commit breaks it:\r\nhttps://github.com/dotnet/msbuild/pull/6023/commits/9b123c40e5e7d729b07854c1a62c14bf50c2208c\r\n\r\nA lock around the method doesn't help.\r\n\r\nI'm stumped! Will go for a walk and try to attack it tomorrow.",
        "createdAt": "2021-01-12T06:00:52Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "Hey @dsplaisted do you have any wisdom for me? This commit is trying to reuse the MemoryStream:\r\nhttps://github.com/dotnet/msbuild/commit/9b123c40e5e7d729b07854c1a62c14bf50c2208c\r\n\r\nand it breaks Mono, see build failures. I've isolated it to this change. I'd try a pooled memory stream but first I want to understand what is going on? Are two of these methods running simultaneously? Why does the lock not help?",
        "createdAt": "2021-01-12T06:07:41Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "It looks like the initial commit is your first attempt and then you're generally just backing off the changes to try to get it to work?\r\n\r\nI'm not sure why it's not working.  I don't think multiple reads or writes should be going on at the same time in a single process.  Maybe a read and write can be happening at the same time and that's causing an issue?  But I don't know.",
        "createdAt": "2021-01-12T23:08:28Z",
        "author": {
          "login": "dsplaisted"
        }
      },
      {
        "body": "What if you make the fields thread local? Every thread would get its own cached byte array and avoid data races. If that fixes the problem it means that that method is indeed multi threaded, which is a surprise",
        "createdAt": "2021-01-14T01:25:28Z",
        "author": {
          "login": "cdmihai"
        }
      },
      {
        "body": "The command line I use locally for testing on Mac:\r\n\r\n```\r\neng/cibuild_bootstrapped_msbuild.sh --host_type mono /p:SuppressLicenseValidation=true\r\n```",
        "createdAt": "2021-01-17T04:12:06Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "Here's the raw MSBuild invocation:\r\n\r\n```\r\n./build.sh /p:CreateBootstrap=true /p:MonoBuild=true\r\n\r\nmono /Users/kirill/MSB/artifacts/bin/bootstrap/net472/MSBuild/Current/Bin/MSBuild.dll /bl /clp:Summary /m /nr:false /p:ContinuousIntegrationBuild=true /p:Configuration=Debug-MONO /p:RepoRoot=/Users/kirill/MSB /p:Restore=true /p:Build=true /p:Rebuild=false /p:Pack=false /p:IntegrationTest=false /p:PerformanceTest=false /p:Sign=false /p:Publish=false /p:CreateBootstrap=false /p:SuppressLicenseValidation=true /p:MSBuildExtensionsPath=/Users/kirill/MSB/stage1/bin/bootstrap/net472/MSBuild/Current/Bin/Extensions /p:MSBuildExtensionsPath32=/Users/kirill/MSB/stage1/bin/bootstrap/net472/MSBuild/Current/Bin/Extensions /p:MSBuildExtensionsPath64=/Users/kirill/MSB/stage1/bin/bootstrap/net472/MSBuild/Current/Bin/Extensions /p:DeterministicSourcePaths=false /v:minimal /warnaserror /Users/kirill/.nuget/packages/microsoft.dotnet.arcade.sdk/1.0.0-beta.20580.3/tools/Build.proj\r\n```",
        "createdAt": "2021-01-17T04:19:23Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "OK I understand what's happening:\r\n\r\nthis async call doesn't await and returns:\r\nhttps://github.com/dotnet/msbuild/blob/34bbbedaf415f4bd185a0ef4d7e67dbd633f4ca6/src/Build/BackEnd/Components/Communications/NodeProviderOutOfProcBase.cs#L762\r\n\r\nand then since the array is reused the Unix implementation of PipeStream trips over itself:\r\nhttps://github.com/mono/corefx/blob/7c24bb0756fd39fbf09b8777f25c15d21d78eb46/src/System.IO.Pipes/src/System/IO/Pipes/PipeStream.Unix.cs#L263\r\n\r\nBecause we queue a new call to SendAsync before the previous call returns and frees up our array.\r\n\r\nSo we can't reuse the array if we don't await on line 762 or use the synchronous Write method on the NamedPipeClientStream.\r\n",
        "createdAt": "2021-01-17T06:54:53Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "OK folks, note one significant change: on Mono (== when FEATURE_APM is not set) we switch the last write of the packet to be synchronous as well:\r\nhttps://github.com/dotnet/msbuild/pull/6023/commits/6eb445dc4128819b50094b37760388a135f28452\r\n\r\nThe other codepath (used by Windows) doesn't seem to have a problem sharing the array because it ends up in a WriteFileNative call where we fully read the array segment we're passed before returning from BeginWrite.\r\n\r\nOn Mono it used to be a fire-and-forget async task (WriteAsync was not awaited), and it led to new writes being called when the previous write didn't return. Not only it gets in the way of reusing the array, but more importantly I think it can explain some weirdness I saw with Mono parallel builds: https://github.com/mono/mono/issues/20669\r\n\r\nSince the Mono parallel build is already slow enough, I think making this change won't make things much worse. There are much more impactful things we can do to improve perf there.",
        "createdAt": "2021-01-17T18:38:22Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "@KirillOsenkov Great job on figuring out what was wrong on Mono.\r\n\r\nHowever, it seems like we're relying on the implementation of `BeginWrite` to copy the data out of the buffer we pass before returning.  Is it safe to assume that it will do that?  Should we instead switch to a buffer pool where the buffer is released in `PacketWriteComplete`?",
        "createdAt": "2021-01-18T02:55:32Z",
        "author": {
          "login": "dsplaisted"
        }
      },
      {
        "body": "I had the same concern.\r\n\r\nIt would make things much simpler if the last write was synchronous as well. Are we sure that making that last write fire-and-forget async is significant for perf?",
        "createdAt": "2021-01-18T04:17:08Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "> Are we sure that making that last write fire-and-forget async is significant for perf?\r\n\r\nGiven the excellent documentation, there's no way to know but to try it out. You can try and build orchard core with /m and do an RPS run. My gut feeling is that it won't affect perf too much because most packets are fairly short. The biggest ones that I'm aware of are ProjectInstance and BuildResult objects, and there's only two of those per built project. ProjectInstance objects are not sent during cmdline builds, only in VS builds, so running RPS would be good. Logging packets are the most frequent, but how large can those be? \ud83d\ude0a. And besides, AFAIK packet sending happens on a separate thread than the RequestBuilder so it won't hold up builds, worst case it will congest the send queue.\r\n\r\n",
        "createdAt": "2021-01-18T16:57:28Z",
        "author": {
          "login": "cdmihai"
        }
      },
      {
        "body": "So I ran some numbers on a medium-size fully built incremental solution (so the C# compiler is never invoked).\r\n\r\nmaster: | memoryStream: | sync:\r\n-- | -- | --\r\n21.699 | 19.871 | 23.905\r\n22.32 | 16.993 | 19.276\r\n20.119 | 21.856 | 21.166\r\n20.414 | 21.569 | 20.705\r\n22.196 | 19.274 | 19.269\r\n\r\nFirst column is current master. Second is the PR before I made the writing synchronous. The last one is the last commit where I write the entire packet synchronously. Of course this is just a toy experiment and we'd need to run RPS but this confirms my intuition that making things sync here likely won't create much of contention.\r\n\r\n@cdmihai logging packets can easily get huge, ~ 5MB, for logging of task inputs, task outputs, item adds and removes (all items with metadata are concatenated into a giant 5MB string and sent across the wire in a single BuildMessageEventArgs). You can see some stats here: \r\nhttps://github.com/KirillOsenkov/MSBuildStructuredLog/wiki/BinLog-Stats\r\n\r\nHere's a sample part of a single packet sent by RAR:\r\nhttps://gist.github.com/KirillOsenkov/8aa84151b92b3837dbd24227e834df58 :)",
        "createdAt": "2021-01-18T19:04:57Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "This is not ready yet - we have a real perf regression identified by @cdmihai:\r\n\r\n```\r\n70d3538\r\n21.79\r\n22.31\r\n22.13\r\n22.41\r\n20.58\r\n \r\n6eb445d (Write the last part of the packet synchronously on Mono.)\r\n22.94\r\n20.68\r\n21.03\r\n22.46\r\n21.54\r\n\r\ndc5f1fb7 (Write the entire packet in SendData() synchronously.)\r\n25.29\r\n26.14\r\n23.74\r\n25.42\r\n23.57\r\n```\r\n\r\nAdditionally, we have an RPS failure, because we now preallocate two 1 MB byte arrays per worker node, which RPS caught:\r\n![image](https://user-images.githubusercontent.com/679326/105430379-30041600-5c08-11eb-8544-b96f6605effd.png)\r\n\r\nI'll need to confirm the overall memory savings on larger builds and if it is as significant as I saw, we'll need to ask for an RPS exception to allow us to take an initial hit, in exchange for almost no byte[] allocations down the line in this area.\r\n\r\nAlso I'll need to look if I can make the entire SendData() async (fire-and-forget) so that we don't block on it finishing. If switching to sync for the last part of the packet has caused such a dramatic slowdown maybe we can squeeze more out of it if we move the entire SendData() call to a different async queue to queue up a packet and return immediately.",
        "createdAt": "2021-01-22T00:48:38Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "> Additionally, we have an RPS failure, because we now preallocate two 1 MB byte arrays per worker node\r\n\r\nWould it make sense to start with smaller arrays then and pay the resizing cost in scenarios where a lot of data is moved between nodes?",
        "createdAt": "2021-01-22T17:01:10Z",
        "author": {
          "login": "ladipro"
        }
      },
      {
        "body": "@ladipro certainly a simple way to trick RPS ;) I'll try that.",
        "createdAt": "2021-01-22T22:27:58Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "reminds me of Volkswagen for some reason",
        "createdAt": "2021-01-22T22:28:19Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "I'm experimenting with moving SendData() off the main loop (to keep `BuildManager.ProcessWorkQueue()` unblocked).\r\nSo far I'm seeing decent numbers (slightly better than anything seen so far).\r\n\r\nMeasuring this is incredibly tricky. I've spent two full days trying out various approaches, measuring various codebases, with various flags (/bl, /graph), debug vs. release, last write sync vs. async, MemoryStreamPool vs. a single memory stream, attempts at optimizing DrainPacketQueue(), etc. The numbers fluctuate and are highly inconclusive. There would be outliers and no clear picture. But the trend does show a little bit, so at least you can compare each pair of approaches.\r\n\r\nThis is a rough draft, and if the approach looks OK I'll rebase and cleanup the code and add comments and such.\r\n\r\nHere are my crazy numbers from the last two days:\r\n![image](https://user-images.githubusercontent.com/679326/105656955-431d1d00-5e78-11eb-8f9a-70f1b3677eca.png)\r\n",
        "createdAt": "2021-01-25T03:13:43Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "Here are the final numbers I got that matter the most:\r\n\r\n70d3538 | 6eb445d | c89545c\r\n-- | -- | --\r\n23.817 | 22.403 | 21.04\r\n22.167 | 22.893 | 19.805\r\n21.023 | 20.327 | 20.531\r\n22.526 | 21.806 | 20.294\r\n18.419 | 21.037 | 18.998\r\n19.652 | 19.931 | 20.176\r\n\r\n70d3538 is the base commit of this PR. 6eb445d reuses MemoryStreams. c89545c is the last commit which makes SendData return immediately and writes the packets on a separate thread asynchronously.",
        "createdAt": "2021-01-25T03:17:27Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "I pushed this branch to exp/kirillo/memoryStream to see if RPS is more happy this way",
        "createdAt": "2021-01-25T04:56:12Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "Unfortunately this is a no-go for now. On a single threaded build this PR is noticeably slower than master:\r\n\r\nmaster | #6023\r\n-- | --\r\n32.377 | 38.573\r\n31.758 | 34.411\r\n31.481 | 33.057\r\n32.62 | 33.376\r\n32.495 | 32.679\r\n32.6 | 33.203\r\n\r\nI'll need to profile and investigate why.",
        "createdAt": "2021-01-26T05:38:31Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "Hmm, I've remeasured again and now it's on par with master for single-node:\r\n\r\n32.847\r\n32.151\r\n32.023\r\n32.876\r\n32.028\r\n\r\nI profiled and saw nothing under profiler that I touch. I even turned off Defender for the path.",
        "createdAt": "2021-01-26T05:55:11Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "Without /bl and with /m the numbers are ever so slightly more favorable for this PR:\r\n\r\nmaster | #6023\r\n-- | --\r\n11.324 | 11.057\r\n11.809 | 10.743\r\n10.833 | 10.339\r\n12.028 | 11.245\r\n10.762 | 10.976\r\n10.359 | 10.931\r\n\r\n",
        "createdAt": "2021-01-26T19:40:13Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "This should be ready for final review.",
        "createdAt": "2021-01-26T20:02:58Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "@ladipro yup, Tasks were designed to point forward (to continuations), but not backward (a Task doesn\u2019t know what started it and doesn\u2019t hold on to predecessors). So the chain would just fall off and get GCed.",
        "createdAt": "2021-01-29T19:23:19Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "@KirillOsenkov This looks awesome. What's holding this up from getting in? Are we targeting 16.9 or 16.10?",
        "createdAt": "2021-02-08T03:56:14Z",
        "author": {
          "login": "davkean"
        }
      },
      {
        "body": "16.10. I\u2019m waiting for you guys to merge this :)",
        "createdAt": "2021-02-08T05:38:47Z",
        "author": {
          "login": "KirillOsenkov"
        }
      },
      {
        "body": "Is this targeting 16.10 because of the risk? ",
        "createdAt": "2021-02-08T06:14:44Z",
        "author": {
          "login": "davkean"
        }
      },
      {
        "body": "Yeah, not enough runway for 16.9 to get some thorough testing and dogfooding time",
        "createdAt": "2021-02-08T08:33:45Z",
        "author": {
          "login": "KirillOsenkov"
        }
      }
    ]
  },
  "reviewThreads": {
    "nodes": [
      {
        "comments": {
          "nodes": [
            {
              "body": "This `if` looks redundant now, both branches do the same thing.",
              "createdAt": "2021-01-21T23:18:22Z",
              "path": "src/Build/BackEnd/Components/Communications/NodeProviderOutOfProcBase.cs",
              "diffHunk": "@@ -733,34 +741,32 @@ public async Task RunPacketReadLoopAsync()\n             /// <param name=\"packet\">The packet to send.</param>\n             public void SendData(INodePacket packet)\n             {\n-                MemoryStream writeStream = new MemoryStream();\n-                ITranslator writeTranslator = BinaryTranslator.GetWriteTranslator(writeStream);\n+                // clear the buffer but keep the underlying capacity to avoid reallocations\n+                _writeBufferMemoryStream.SetLength(0);\n+\n+                ITranslator writeTranslator = BinaryTranslator.GetWriteTranslator(_writeBufferMemoryStream);\n                 try\n                 {\n-                    writeStream.WriteByte((byte)packet.Type);\n+                    _writeBufferMemoryStream.WriteByte((byte)packet.Type);\n \n                     // Pad for the packet length\n-                    writeStream.Write(BitConverter.GetBytes((int)0), 0, 4);\n+                    _writeBufferStreamWriter.Write(0);\n                     packet.Translate(writeTranslator);\n \n+                    int writeStreamLength = (int)_writeBufferMemoryStream.Position;\n+\n                     // Now plug in the real packet length\n-                    writeStream.Position = 1;\n-                    writeStream.Write(BitConverter.GetBytes((int)writeStream.Length - 5), 0, 4);\n+                    _writeBufferMemoryStream.Position = 1;\n+                    _writeBufferStreamWriter.Write(writeStreamLength - 5);\n \n-                    byte[] writeStreamBuffer = writeStream.GetBuffer();\n+                    byte[] writeStreamBuffer = _writeBufferMemoryStream.GetBuffer();\n \n-                    for (int i = 0; i < writeStream.Length; i += MaxPacketWriteSize)\n+                    for (int i = 0; i < writeStreamLength; i += MaxPacketWriteSize)\n                     {\n-                        int lengthToWrite = Math.Min((int)writeStream.Length - i, MaxPacketWriteSize);\n-                        if ((int)writeStream.Length - i <= MaxPacketWriteSize)\n+                        int lengthToWrite = Math.Min(writeStreamLength - i, MaxPacketWriteSize);\n+                        if (writeStreamLength - i <= MaxPacketWriteSize)",
              "author": {
                "login": "ladipro"
              }
            },
            {
              "body": "good point",
              "createdAt": "2021-01-22T00:49:49Z",
              "path": "src/Build/BackEnd/Components/Communications/NodeProviderOutOfProcBase.cs",
              "diffHunk": "@@ -733,34 +741,32 @@ public async Task RunPacketReadLoopAsync()\n             /// <param name=\"packet\">The packet to send.</param>\n             public void SendData(INodePacket packet)\n             {\n-                MemoryStream writeStream = new MemoryStream();\n-                ITranslator writeTranslator = BinaryTranslator.GetWriteTranslator(writeStream);\n+                // clear the buffer but keep the underlying capacity to avoid reallocations\n+                _writeBufferMemoryStream.SetLength(0);\n+\n+                ITranslator writeTranslator = BinaryTranslator.GetWriteTranslator(_writeBufferMemoryStream);\n                 try\n                 {\n-                    writeStream.WriteByte((byte)packet.Type);\n+                    _writeBufferMemoryStream.WriteByte((byte)packet.Type);\n \n                     // Pad for the packet length\n-                    writeStream.Write(BitConverter.GetBytes((int)0), 0, 4);\n+                    _writeBufferStreamWriter.Write(0);\n                     packet.Translate(writeTranslator);\n \n+                    int writeStreamLength = (int)_writeBufferMemoryStream.Position;\n+\n                     // Now plug in the real packet length\n-                    writeStream.Position = 1;\n-                    writeStream.Write(BitConverter.GetBytes((int)writeStream.Length - 5), 0, 4);\n+                    _writeBufferMemoryStream.Position = 1;\n+                    _writeBufferStreamWriter.Write(writeStreamLength - 5);\n \n-                    byte[] writeStreamBuffer = writeStream.GetBuffer();\n+                    byte[] writeStreamBuffer = _writeBufferMemoryStream.GetBuffer();\n \n-                    for (int i = 0; i < writeStream.Length; i += MaxPacketWriteSize)\n+                    for (int i = 0; i < writeStreamLength; i += MaxPacketWriteSize)\n                     {\n-                        int lengthToWrite = Math.Min((int)writeStream.Length - i, MaxPacketWriteSize);\n-                        if ((int)writeStream.Length - i <= MaxPacketWriteSize)\n+                        int lengthToWrite = Math.Min(writeStreamLength - i, MaxPacketWriteSize);\n+                        if (writeStreamLength - i <= MaxPacketWriteSize)",
              "author": {
                "login": "KirillOsenkov"
              }
            }
          ]
        }
      },
      {
        "comments": {
          "nodes": [
            {
              "body": "nit: define \"primary lock\"",
              "createdAt": "2021-01-27T03:12:16Z",
              "path": "src/Build/BackEnd/Components/Communications/NodeProviderOutOfProcBase.cs",
              "diffHunk": "@@ -728,54 +740,80 @@ public async Task RunPacketReadLoopAsync()\n #endif\n \n             /// <summary>\n-            /// Sends the specified packet to this node.\n+            /// Sends the specified packet to this node asynchronously.\n+            /// The method enqueues a task to write the packet and returns\n+            /// immediately. This is because SendData() is on a hot path\n+            /// under the primary lock and we want to minimize our time there.",
              "author": {
                "login": "cdmihai"
              }
            },
            {
              "body": "Added the clarification that it's the BuildManager's _syncLock. ",
              "createdAt": "2021-01-28T22:32:40Z",
              "path": "src/Build/BackEnd/Components/Communications/NodeProviderOutOfProcBase.cs",
              "diffHunk": "@@ -728,54 +740,80 @@ public async Task RunPacketReadLoopAsync()\n #endif\n \n             /// <summary>\n-            /// Sends the specified packet to this node.\n+            /// Sends the specified packet to this node asynchronously.\n+            /// The method enqueues a task to write the packet and returns\n+            /// immediately. This is because SendData() is on a hot path\n+            /// under the primary lock and we want to minimize our time there.",
              "author": {
                "login": "KirillOsenkov"
              }
            }
          ]
        }
      },
      {
        "comments": {
          "nodes": [
            {
              "body": "Cute trick :)",
              "createdAt": "2021-01-27T03:12:52Z",
              "path": "src/Build/BackEnd/Components/Communications/NodeProviderOutOfProcBase.cs",
              "diffHunk": "@@ -728,54 +740,80 @@ public async Task RunPacketReadLoopAsync()\n #endif\n \n             /// <summary>\n-            /// Sends the specified packet to this node.\n+            /// Sends the specified packet to this node asynchronously.\n+            /// The method enqueues a task to write the packet and returns\n+            /// immediately. This is because SendData() is on a hot path\n+            /// under the primary lock and we want to minimize our time there.\n             /// </summary>\n             /// <param name=\"packet\">The packet to send.</param>\n             public void SendData(INodePacket packet)\n             {\n-                MemoryStream writeStream = new MemoryStream();\n+                _packetWriteQueue.Add(packet);\n+                DrainPacketQueue();\n+            }\n+\n+            /// <summary>\n+            /// Schedule a task to drain the packet write queue. We could have had a\n+            /// dedicated thread that would pump the queue constantly, but\n+            /// we don't want to allocate a dedicated thread per node (1MB stack)\n+            /// </summary>\n+            /// <remarks>Usually there'll be a single packet in the queue, but sometimes\n+            /// a burst of SendData comes in, with 10-20 packets scheduled. In this case\n+            /// the first scheduled task will drain all of them, and subsequent tasks\n+            /// will run on an empty queue. I tried to write logic that avoids queueing\n+            /// a new task if the queue is already being drained, but it didn't show any\n+            /// improvement and made things more complicated.</remarks>\n+            private void DrainPacketQueue()\n+            {\n+                // this lock is only necessary to protect a write to _packetWriteDrainTask field\n+                lock (_packetWriteQueue)\n+                {\n+                    // average latency between the moment this runs and when the delegate starts\n+                    // running is about 100-200 microseconds (unless there's thread pool saturation)\n+                    _packetWriteDrainTask = _packetWriteDrainTask.ContinueWith(_ =>",
              "author": {
                "login": "cdmihai"
              }
            },
            {
              "body": "+1 \ud83d\udc4d\r\nAre completed tasks properly released so this endless chaining is not leaking memory? I hope the answer is yes but it may be worth confirming, just in case.  ",
              "createdAt": "2021-01-29T14:14:52Z",
              "path": "src/Build/BackEnd/Components/Communications/NodeProviderOutOfProcBase.cs",
              "diffHunk": "@@ -728,54 +740,80 @@ public async Task RunPacketReadLoopAsync()\n #endif\n \n             /// <summary>\n-            /// Sends the specified packet to this node.\n+            /// Sends the specified packet to this node asynchronously.\n+            /// The method enqueues a task to write the packet and returns\n+            /// immediately. This is because SendData() is on a hot path\n+            /// under the primary lock and we want to minimize our time there.\n             /// </summary>\n             /// <param name=\"packet\">The packet to send.</param>\n             public void SendData(INodePacket packet)\n             {\n-                MemoryStream writeStream = new MemoryStream();\n+                _packetWriteQueue.Add(packet);\n+                DrainPacketQueue();\n+            }\n+\n+            /// <summary>\n+            /// Schedule a task to drain the packet write queue. We could have had a\n+            /// dedicated thread that would pump the queue constantly, but\n+            /// we don't want to allocate a dedicated thread per node (1MB stack)\n+            /// </summary>\n+            /// <remarks>Usually there'll be a single packet in the queue, but sometimes\n+            /// a burst of SendData comes in, with 10-20 packets scheduled. In this case\n+            /// the first scheduled task will drain all of them, and subsequent tasks\n+            /// will run on an empty queue. I tried to write logic that avoids queueing\n+            /// a new task if the queue is already being drained, but it didn't show any\n+            /// improvement and made things more complicated.</remarks>\n+            private void DrainPacketQueue()\n+            {\n+                // this lock is only necessary to protect a write to _packetWriteDrainTask field\n+                lock (_packetWriteQueue)\n+                {\n+                    // average latency between the moment this runs and when the delegate starts\n+                    // running is about 100-200 microseconds (unless there's thread pool saturation)\n+                    _packetWriteDrainTask = _packetWriteDrainTask.ContinueWith(_ =>",
              "author": {
                "login": "ladipro"
              }
            }
          ]
        }
      }
    ]
  }
}